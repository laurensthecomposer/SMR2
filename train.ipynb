{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurensg420/SMR2/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tc9O5ypPjOP",
        "colab_type": "text"
      },
      "source": [
        "Train with GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs3MhwPvm_YC",
        "colab_type": "code",
        "outputId": "ec5aeb49-d933-4b7a-bf74-dedb55052078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os, sys, math\n",
        "import numpy as np\n",
        "if 'google.colab' in sys.modules: # Colab-only Tensorflow version selector\n",
        "  %tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owo4TnYDnJW2",
        "colab_type": "code",
        "outputId": "6c7d7a7f-e310-4baa-a0b2-7837e73c49c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "This error most likely means that this notebook is not configured to use a GPU.  Change this in Notebook Settings via the command palette (cmd/ctrl-shift-P) or the Edit menu.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-943ed1d2286d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;34m'configured to use a GPU.  Change this in Notebook Settings via the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X-9O20WnNeS",
        "colab_type": "code",
        "outputId": "bfd36efd-b6e3-4574-c737-b193db6062ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "\n",
        "base_path = '/content/drive/My Drive/ML Mofos SMR/colab_files/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7oBzeQUQ4z8",
        "colab_type": "code",
        "outputId": "e8e8463d-fa08-4f5c-c102-169575706f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# test writing file on Google Drive\n",
        "with open('/content/drive/My Drive/ML Mofos SMR/colab_files/test.txt', 'w+') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat /content/drive/My\\ Drive/ML\\ Mofos\\ SMR/colab_files/test.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello Google Drive!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FC19P9LdPur",
        "colab_type": "code",
        "outputId": "6f68574d-09e6-470b-9812-1cdfb35388a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "!pip install Keras==2.1.5\n",
        "import keras\n",
        "keras.__version__\n",
        "!pip install keras_squeezenet Keras-Applications==1.0.8 Keras-Preprocessing==1.0.9 -q"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Keras==2.1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/65/e4aff762b8696ec0626a6654b1e73b396fcc8b7cc6b98d78a1bc53b85b48/Keras-2.1.5-py2.py3-none-any.whl (334kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras==2.1.5) (1.17.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.1.5) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras==2.1.5) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras==2.1.5) (1.12.0)\n",
            "Installing collected packages: Keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed Keras-2.1.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▌                          | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 30kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[?25h  Building wheel for keras-squeezenet (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM6R6JOxsdra",
        "colab_type": "code",
        "outputId": "401166b0-aeb9-4a03-9c1b-c182588a6515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        }
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from keras_squeezenet import SqueezeNet\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Activation, Dropout, Convolution2D, GlobalAveragePooling2D\n",
        "from keras.models import Sequential\n",
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "import keras\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    # featurewise_center=False,\n",
        "    # samplewise_center=False,\n",
        "    # featurewise_std_normalization=False,\n",
        "    # samplewise_std_normalization=False,\n",
        "    # zca_whitening=False,\n",
        "    # zca_epsilon=1e-06,\n",
        "    # rotation_range=0,\n",
        "    # width_shift_range=0.0,\n",
        "    # height_shift_range=0.0,\n",
        "    brightness_range=(0.8,1.2), # doc on brightness_range = https://pillow.readthedocs.io/en/3.0.x/reference/ImageEnhance.html#PIL.ImageEnhance.Brightness\n",
        "    # shear_range=0.0,\n",
        "    # zoom_range=0.0,\n",
        "    # channel_shift_range=0.0,\n",
        "    # fill_mode='nearest',\n",
        "    # cval=0.0,\n",
        "    # rotation_range=20,\n",
        "    horizontal_flip=True,\n",
        "    # vertical_flip=False,\n",
        "    # rescale=None,\n",
        "    # preprocessing_function=None,\n",
        "    # data_format='channels_last',\n",
        "    # validation_split=0.0,\n",
        "    # interpolation_order=1,\n",
        "    # dtype='float32'\n",
        "    )\n",
        "\n",
        "# settings\n",
        "train_batch_size = 20\n",
        "val_batch_size = 20\n",
        "roi_square_size = 350\n",
        "epochs = 4\n",
        "\n",
        "# train_datagen = ImageDataGenerator(\n",
        "# rescale=1./255,\n",
        "# shear_range=0.2,\n",
        "# zoom_range=0.2,\n",
        "# horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator()\n",
        "train_it = train_datagen.flow_from_directory(\n",
        "    os.path.join( base_path,'dataset/image_data_blue_light_split/train' ),\n",
        "    target_size=(roi_square_size, roi_square_size),\n",
        "    class_mode='categorical',\n",
        "    batch_size=train_batch_size\n",
        ")\n",
        "\n",
        "val_it = test_datagen.flow_from_directory(\n",
        "    os.path.join( base_path,'dataset/image_data_blue_light_split/validate' ),\n",
        "    target_size=(roi_square_size, roi_square_size),\n",
        "    class_mode='categorical',\n",
        "    batch_size=val_batch_size\n",
        ")\n",
        "\n",
        "amount_classes = len( os.listdir( os.path.join( base_path, 'dataset/image_data_blue_light_split/train' ) ) )\n",
        "\n",
        "\n",
        "amount_train_images = train_it.samples\n",
        "amount_val_images = val_it.samples\n",
        "\n",
        "\n",
        "batches_in_dataset = (amount_train_images / train_batch_size)\n",
        "epochs = int( batches_in_dataset ) * 3\n",
        "\n",
        "\n",
        "model_name = os.path.join( base_path, 'models/image_data_blue_light_split'\\\n",
        "       + '_sq' + str(roi_square_size)\\\n",
        "       + '_e'  + str(epochs)\\\n",
        "       + '_tb' + str(train_batch_size)\\\n",
        "       + '_vb' + str(val_batch_size)\\\n",
        "       + '_aug-hor-briran0_8;1_2'\\\n",
        "       +'.h5')\n",
        "\n",
        "print( \"batches_in_dataset:\", batches_in_dataset)\n",
        "print( \"epochs:\", epochs )\n",
        "print( \"train_batch_size:\", train_batch_size)\n",
        "print( \"val_batch_size:\", val_batch_size )\n",
        "print( \"model_name:\", model_name)\n",
        "\n",
        "def get_model(amount_classes):\n",
        "    model = Sequential( [\n",
        "        # change model res\n",
        "        SqueezeNet( input_shape=(roi_square_size, roi_square_size, 3), include_top=False ),\n",
        "        Convolution2D( amount_classes, (1, 1), padding='valid' ),\n",
        "        Activation( 'relu' ),\n",
        "        GlobalAveragePooling2D(),\n",
        "        Activation( 'softmax' )\n",
        "    ] )\n",
        "    return model\n",
        "\n",
        "import keras.backend as K\n",
        "\n",
        "# define the model\n",
        "model = get_model( amount_classes )\n",
        "model.compile(\n",
        "    optimizer=Adam( lr=0.0001 ),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['acc',]\n",
        ")  # lr = learning rate\n",
        "\n",
        "\n",
        "# keras.utils.plot_model(SqueezeNet( input_shape=(roi_square_size, roi_square_size, 3), include_top=False ), 'multi_input_and_output_model.png', show_shapes=True)\n",
        "# # save checkpoints after epochs\n",
        "filepath=os.path.join(base_path,\"weights-improvement-e{epoch:02d}-va{val_acc:.2f}.h5\")\n",
        "# print(filepath)\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=False, mode='max', period=5)\n",
        "\n",
        "csv_logger = CSVLogger(model_name.split('.')[0]+'.csv')\n",
        "\n",
        "callbacks_list = [csv_logger]\n",
        "\n",
        "\n",
        "# start training\n",
        "with tf.device('/device:GPU:0'):\n",
        "  history = model.fit_generator( train_it, steps_per_epoch=2, validation_data=val_it,\n",
        "                                  validation_steps=2, epochs=epochs, verbose=1, callbacks = callbacks_list)\n",
        "# model.fit_generator(train_it, steps_per_epoch=10, validation_data=val_it, validation_steps=1, epochs=epochs,\n",
        "# verbose=1)\n",
        "# print(model_name)\n",
        "# save the model for later use\n",
        "# model.save( model_name )\n",
        "# score = model.evaluate(np.array(data), np.array(labels))\n",
        "# print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 490 images belonging to 9 classes.\n",
            "Found 167 images belonging to 9 classes.\n",
            "batches_in_dataset: 24.5\n",
            "epochs: 72\n",
            "train_batch_size: 20\n",
            "val_batch_size: 20\n",
            "model_name: /content/drive/My Drive/ML Mofos SMR/colab_files/models/image_data_blue_light_split_sq350_e72_tb20_vb20_aug-hor-briran0_8;1_2.h5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:507: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3831: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3655: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/rcmalli/keras-squeezenet/releases/download/v1.0/squeezenet_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "3039232/3032184 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:167: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:179: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:183: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:192: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3008: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:976: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:963: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/72\n",
            "2/2 [==============================] - 34s 17s/step - loss: 3.0846 - acc: 0.0750 - val_loss: 2.4714 - val_acc: 0.1500\n",
            "Epoch 2/72\n",
            "1/2 [==============>...............] - ETA: 3s - loss: 2.5455 - acc: 0.0000e+00"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ah1156R3PVh",
        "colab_type": "code",
        "outputId": "e12876ae-c080-42fb-ac83-84f1ae219cbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(history.history)\n",
        "print(df.head())\n",
        "csv_path = model_name.split('.')[0]+'.csv'\n",
        "print(csv_path)\n",
        "df.to_csv(model_name.split('.')[0]+'.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0f33dc51dc3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcsv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RAa_jQp6ZcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sq_model = SqueezeNet( input_shape=(roi_square_size, roi_square_size, 3), include_top=False )\n",
        "sq_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q97mYfupmLyB",
        "colab_type": "code",
        "outputId": "4de4e32a-5b63-45ac-bbc2-09cdb68f5173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# verbose=1)\n",
        "print(model_name)\n",
        "# save the model for later use\n",
        "model.save( model_name )\n",
        "# score = model.evaluate(np.array(data), np.array(labels))\n",
        "# print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ML Mofos SMR/colab_files/models/image_data_blue_light_split_sq350_e72_tb20_vb20_aug-hor-rotran20-briran0_8;1_2.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvyN_vbdWxal",
        "colab_type": "code",
        "outputId": "c8cf106f-e3d1-48cb-e713-76f9e00e5988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-rKwUWOvnWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file=base_path+'model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF_kPyBjvroY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}